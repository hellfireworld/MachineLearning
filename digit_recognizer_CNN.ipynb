{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_recognizer_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSKAd/T+F6Yu7nl5MDBnXP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellfireworld/MachineLearning/blob/master/digit_recognizer_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqY1kdco2Lo5",
        "colab_type": "text"
      },
      "source": [
        "***Alexandros_Tsevrenis_MTN1914***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XmWmaxg2WPT",
        "colab_type": "text"
      },
      "source": [
        "# **Import essential libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjf7ccYzbeDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sys\n",
        "import io\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import IPython\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "#pytorch utility imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.utils import make_grid\n",
        "#neural net imports\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_eCpKXg2erq",
        "colab_type": "text"
      },
      "source": [
        "# **Import day.csv file with GOOGLE_COLAB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKWZmVw1bpc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNlZdL5B4pbb",
        "colab_type": "code",
        "outputId": "9ac1e0e4-73aa-47b0-8239-e64be4ef16de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaGkGneK6eS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_train = r'/gdrive/My Drive/datasets/digit_recognizer_dataset_train.csv'\n",
        "file_test = r'/gdrive/My Drive/datasets/digit_recognizer_dataset_test.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Woqdoc6t7Wb2",
        "colab_type": "code",
        "outputId": "2a36ef62-f121-4f40-c3a1-592100985509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "df = pd.read_csv(file_train)\n",
        "df2 = pd.read_csv(file_test)\n",
        "print(df)\n",
        "print(df2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
            "0          1       0       0       0  ...         0         0         0         0\n",
            "1          0       0       0       0  ...         0         0         0         0\n",
            "2          1       0       0       0  ...         0         0         0         0\n",
            "3          4       0       0       0  ...         0         0         0         0\n",
            "4          0       0       0       0  ...         0         0         0         0\n",
            "...      ...     ...     ...     ...  ...       ...       ...       ...       ...\n",
            "27995      6       0       0       0  ...         0         0         0         0\n",
            "27996      0       0       0       0  ...         0         0         0         0\n",
            "27997      8       0       0       0  ...         0         0         0         0\n",
            "27998      0       0       0       0  ...         0         0         0         0\n",
            "27999      7       0       0       0  ...         0         0         0         0\n",
            "\n",
            "[28000 rows x 785 columns]\n",
            "       pixel0  pixel1  pixel2  pixel3  ...  pixel780  pixel781  pixel782  pixel783\n",
            "0           0       0       0       0  ...         0         0         0         0\n",
            "1           0       0       0       0  ...         0         0         0         0\n",
            "2           0       0       0       0  ...         0         0         0         0\n",
            "3           0       0       0       0  ...         0         0         0         0\n",
            "4           0       0       0       0  ...         0         0         0         0\n",
            "...       ...     ...     ...     ...  ...       ...       ...       ...       ...\n",
            "13995       0       0       0       0  ...         0         0         0         0\n",
            "13996       0       0       0       0  ...         0         0         0         0\n",
            "13997       0       0       0       0  ...         0         0         0         0\n",
            "13998       0       0       0       0  ...         0         0         0         0\n",
            "13999       0       0       0       0  ...         0         0         0         0\n",
            "\n",
            "[14000 rows x 784 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu53LMiW2ovo",
        "colab_type": "text"
      },
      "source": [
        "# **Read the dataset from the CSV file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzT5w0972x4I",
        "colab_type": "text"
      },
      "source": [
        "**Define df variable as day.csv using pandas library.\n",
        "Read separated values (csv) file into DataFrame.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sYXntfqcHiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(io.StringIO(uploaded['digit_recognizer_dataset.csv'].decode('utf-8')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3OWEpqt4RSW",
        "colab_type": "text"
      },
      "source": [
        "# **Print the shape of our whole data (rows, columns)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XegB6RGxtoD",
        "colab_type": "code",
        "outputId": "c24c2e92-a3db-4828-f8bc-4fefabec4a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGYWZy8P45UU",
        "colab_type": "text"
      },
      "source": [
        "# **Firstly, split the pixelX features from the first column and store them in X, secondly store the first column as the prediction feature/variable as y**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNYNPswI6WbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:, 1:]\n",
        "y = df['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPjuogPK8OFX",
        "colab_type": "text"
      },
      "source": [
        "# **Plot Digit Three from dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LigJRss8Ela",
        "colab_type": "code",
        "outputId": "15e57e59-da0b-48aa-8c35-291691401b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "#three\n",
        "three = df.iloc[25, 1:]\n",
        "#print(three.shape)\n",
        "three = three.values.reshape(28, 28)\n",
        "plt.imshow(three, cmap='gray')\n",
        "plt.title(\"Digit 3\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2M0y4bCbZcJhN8uEw26WCYfdLBP/D4KpF7Tzep6nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9lf5Qdo73g8",
        "colab_type": "code",
        "outputId": "471be6d5-ced6-474d-c2b2-0cae71126d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # stops the freeze error\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    print('loop')\n",
        "\n",
        "    # GPU CUDA\n",
        "    # check cuda version\n",
        "    #print(torch.cuda.is_available())\n",
        "    #print(torch.backends.cudnn.enabled)\n",
        "    #if torch.cuda.is_available():\n",
        "    #    device = torch.device('cuda')\n",
        "    #print(device)\n",
        "\n",
        "    train_df = df\n",
        "    test_df = df2\n",
        "    #train_df = df.iloc[:28000, :]\n",
        "    #test_df = df.iloc[28000:, :]\n",
        "\n",
        "    train_labels = train_df['label'].values\n",
        "    train_images = (train_df.iloc[:, 1:].values).astype('float32')\n",
        "    test_images = (test_df.iloc[:, :].values).astype('float32')\n",
        "\n",
        "    # Training and Validation Split\n",
        "    train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n",
        "                                                                          stratify=train_labels, random_state=123,\n",
        "                                                                          test_size=0.20)\n",
        "    train_images = train_images.reshape(train_images.shape[0], 28, 28)\n",
        "    val_images = val_images.reshape(val_images.shape[0], 28, 28)\n",
        "    test_images = test_images.reshape(test_images.shape[0], 28, 28)\n",
        "\n",
        "    # train samples\n",
        "    for i in range(6, 9):\n",
        "        plt.subplot(330 + (i + 1))\n",
        "        plt.imshow(train_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n",
        "        plt.title(train_labels[i])\n",
        "\n",
        "    # test samples\n",
        "    for i in range(6, 9):\n",
        "        plt.subplot(330 + (i + 1))\n",
        "        plt.imshow(test_images[i].squeeze(), cmap=plt.get_cmap('gray'))\n",
        "\n",
        "\n",
        "    # train\n",
        "    train_images_tensor = torch.tensor(train_images) / 255.0\n",
        "    train_labels_tensor = torch.tensor(train_labels)\n",
        "    train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "\n",
        "    # val\n",
        "    val_images_tensor = torch.tensor(val_images) / 255.0\n",
        "    val_labels_tensor = torch.tensor(val_labels)\n",
        "    val_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "\n",
        "    # test\n",
        "    test_images_tensor = torch.tensor(test_images) / 255.0\n",
        "\n",
        "    train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)\n",
        "    val_loader = DataLoader(val_tensor, batch_size=16, num_workers=2, shuffle=True)\n",
        "    test_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        img_grid = make_grid(data[0:8, ].unsqueeze(1), nrow=8)\n",
        "        img_target_labels = target[0:8, ].numpy()\n",
        "        break\n",
        "\n",
        "    plt.imshow(img_grid.numpy().transpose((1, 2, 0)))\n",
        "    plt.rcParams['figure.figsize'] = (10, 2)\n",
        "    plt.title(img_target_labels, size=16)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "\n",
        "            self.conv_block = nn.Sequential(\n",
        "                nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            )\n",
        "\n",
        "            self.linear_block = nn.Sequential(\n",
        "                nn.Dropout(p=0.5),\n",
        "                nn.Linear(128 * 7 * 7, 128),\n",
        "                nn.BatchNorm1d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(128, 64),\n",
        "                nn.BatchNorm1d(64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(64, 10)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.conv_block(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.linear_block(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "    conv_model = Net()\n",
        "    print(conv_model)\n",
        "\n",
        "    optimizer = optim.Adam(params=conv_model.parameters(), lr=0.003)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "    # GPU CUDA\n",
        "    #if torch.cuda.is_available():\n",
        "    #    conv_model = conv_model.cuda()\n",
        "    #    criterion = criterion.cuda()\n",
        "\n",
        "\n",
        "    def train_model(num_epoch):\n",
        "        conv_model.train()\n",
        "        exp_lr_scheduler.step()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.unsqueeze(1)\n",
        "            data, target = data, target\n",
        "\n",
        "            # GPU CUDA\n",
        "            #if torch.cuda.is_available():\n",
        "            #    data = data.cuda()\n",
        "            #    target = target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = conv_model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if( (batch_idx + 1) % 100 == 0 ):\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    num_epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                               100. * (batch_idx + 1) / len(train_loader), loss.data))\n",
        "\n",
        "\n",
        "    def evaluate(data_loader):\n",
        "        conv_model.eval()\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for data, target in data_loader:\n",
        "            data = data.unsqueeze(1)\n",
        "            data, target = data, target\n",
        "\n",
        "            # GPU CUDA\n",
        "            #if torch.cuda.is_available():\n",
        "            #    data = data.cuda()\n",
        "            #    target = target.cuda()\n",
        "\n",
        "            output = conv_model(data)\n",
        "\n",
        "            loss += F.cross_entropy(output, target, size_average=False).data\n",
        "\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "        loss /= len(data_loader.dataset)\n",
        "\n",
        "        print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(loss, correct, len(data_loader.dataset),\n",
        "                                                                                   100. * correct / len(data_loader.dataset)))\n",
        "\n",
        "    num_epochs = 25\n",
        "\n",
        "    for n in range(num_epochs):\n",
        "        train_model(n)\n",
        "        evaluate(val_loader)\n",
        "\n",
        "    def make_predictions(data_loader):\n",
        "        conv_model.eval()\n",
        "        test_preds = torch.LongTensor()\n",
        "\n",
        "        for i, data in enumerate(data_loader):\n",
        "            data = data.unsqueeze(1)\n",
        "\n",
        "            # GPU CUDA\n",
        "            #if torch.cuda.is_available():\n",
        "            #    data = data.cuda()\n",
        "\n",
        "            output = conv_model(data)\n",
        "\n",
        "            preds = output.cpu().data.max(1, keepdim=True)[1]\n",
        "            test_preds = torch.cat((test_preds, preds), dim=0)\n",
        "\n",
        "        return test_preds\n",
        "\n",
        "    test_set_preds = make_predictions(test_loader)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loop\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if s != self._text:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAABPCAYAAACDIE3ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw2v/HyODmyrsUi4GAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x144 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_block): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=6272, out_features=128, bias=True)\n",
            "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Dropout(p=0.5, inplace=False)\n",
            "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Dropout(p=0.5, inplace=False)\n",
            "    (9): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1600/22400 (7%)]\tLoss: 0.831007\n",
            "Train Epoch: 0 [3200/22400 (14%)]\tLoss: 0.552067\n",
            "Train Epoch: 0 [4800/22400 (21%)]\tLoss: 0.348447\n",
            "Train Epoch: 0 [6400/22400 (29%)]\tLoss: 0.484707\n",
            "Train Epoch: 0 [8000/22400 (36%)]\tLoss: 0.366096\n",
            "Train Epoch: 0 [9600/22400 (43%)]\tLoss: 0.181640\n",
            "Train Epoch: 0 [11200/22400 (50%)]\tLoss: 0.418830\n",
            "Train Epoch: 0 [12800/22400 (57%)]\tLoss: 0.102713\n",
            "Train Epoch: 0 [14400/22400 (64%)]\tLoss: 0.309450\n",
            "Train Epoch: 0 [16000/22400 (71%)]\tLoss: 0.929435\n",
            "Train Epoch: 0 [17600/22400 (79%)]\tLoss: 0.089448\n",
            "Train Epoch: 0 [19200/22400 (86%)]\tLoss: 1.288905\n",
            "Train Epoch: 0 [20800/22400 (93%)]\tLoss: 0.377212\n",
            "Train Epoch: 0 [22400/22400 (100%)]\tLoss: 0.149497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average Val Loss: 0.0703, Val Accuracy: 5475/5600 (97.768%)\n",
            "\n",
            "Train Epoch: 1 [1600/22400 (7%)]\tLoss: 0.303034\n",
            "Train Epoch: 1 [3200/22400 (14%)]\tLoss: 0.465056\n",
            "Train Epoch: 1 [4800/22400 (21%)]\tLoss: 0.176357\n",
            "Train Epoch: 1 [6400/22400 (29%)]\tLoss: 0.037825\n",
            "Train Epoch: 1 [8000/22400 (36%)]\tLoss: 0.226241\n",
            "Train Epoch: 1 [9600/22400 (43%)]\tLoss: 0.057874\n",
            "Train Epoch: 1 [11200/22400 (50%)]\tLoss: 0.178591\n",
            "Train Epoch: 1 [12800/22400 (57%)]\tLoss: 0.139840\n",
            "Train Epoch: 1 [14400/22400 (64%)]\tLoss: 0.083368\n",
            "Train Epoch: 1 [16000/22400 (71%)]\tLoss: 0.088172\n",
            "Train Epoch: 1 [17600/22400 (79%)]\tLoss: 0.077741\n",
            "Train Epoch: 1 [19200/22400 (86%)]\tLoss: 0.432149\n",
            "Train Epoch: 1 [20800/22400 (93%)]\tLoss: 0.162841\n",
            "Train Epoch: 1 [22400/22400 (100%)]\tLoss: 0.026503\n",
            "\n",
            "Average Val Loss: 0.0552, Val Accuracy: 5508/5600 (98.357%)\n",
            "\n",
            "Train Epoch: 2 [1600/22400 (7%)]\tLoss: 0.048504\n",
            "Train Epoch: 2 [3200/22400 (14%)]\tLoss: 0.227547\n",
            "Train Epoch: 2 [4800/22400 (21%)]\tLoss: 0.078484\n",
            "Train Epoch: 2 [6400/22400 (29%)]\tLoss: 0.112821\n",
            "Train Epoch: 2 [8000/22400 (36%)]\tLoss: 0.034040\n",
            "Train Epoch: 2 [9600/22400 (43%)]\tLoss: 0.233511\n",
            "Train Epoch: 2 [11200/22400 (50%)]\tLoss: 0.043254\n",
            "Train Epoch: 2 [12800/22400 (57%)]\tLoss: 0.487573\n",
            "Train Epoch: 2 [14400/22400 (64%)]\tLoss: 0.057414\n",
            "Train Epoch: 2 [16000/22400 (71%)]\tLoss: 0.279467\n",
            "Train Epoch: 2 [17600/22400 (79%)]\tLoss: 0.188083\n",
            "Train Epoch: 2 [19200/22400 (86%)]\tLoss: 0.019899\n",
            "Train Epoch: 2 [20800/22400 (93%)]\tLoss: 0.226757\n",
            "Train Epoch: 2 [22400/22400 (100%)]\tLoss: 0.088091\n",
            "\n",
            "Average Val Loss: 0.0436, Val Accuracy: 5522/5600 (98.607%)\n",
            "\n",
            "Train Epoch: 3 [1600/22400 (7%)]\tLoss: 0.096638\n",
            "Train Epoch: 3 [3200/22400 (14%)]\tLoss: 0.367950\n",
            "Train Epoch: 3 [4800/22400 (21%)]\tLoss: 0.062254\n",
            "Train Epoch: 3 [6400/22400 (29%)]\tLoss: 0.019366\n",
            "Train Epoch: 3 [8000/22400 (36%)]\tLoss: 0.096292\n",
            "Train Epoch: 3 [9600/22400 (43%)]\tLoss: 0.307415\n",
            "Train Epoch: 3 [11200/22400 (50%)]\tLoss: 0.067818\n",
            "Train Epoch: 3 [12800/22400 (57%)]\tLoss: 0.038846\n",
            "Train Epoch: 3 [14400/22400 (64%)]\tLoss: 0.022755\n",
            "Train Epoch: 3 [16000/22400 (71%)]\tLoss: 0.047703\n",
            "Train Epoch: 3 [17600/22400 (79%)]\tLoss: 0.105987\n",
            "Train Epoch: 3 [19200/22400 (86%)]\tLoss: 0.123476\n",
            "Train Epoch: 3 [20800/22400 (93%)]\tLoss: 0.359269\n",
            "Train Epoch: 3 [22400/22400 (100%)]\tLoss: 0.076574\n",
            "\n",
            "Average Val Loss: 0.0429, Val Accuracy: 5526/5600 (98.679%)\n",
            "\n",
            "Train Epoch: 4 [1600/22400 (7%)]\tLoss: 0.264061\n",
            "Train Epoch: 4 [3200/22400 (14%)]\tLoss: 0.160058\n",
            "Train Epoch: 4 [4800/22400 (21%)]\tLoss: 0.059967\n",
            "Train Epoch: 4 [6400/22400 (29%)]\tLoss: 0.040258\n",
            "Train Epoch: 4 [8000/22400 (36%)]\tLoss: 0.209999\n",
            "Train Epoch: 4 [9600/22400 (43%)]\tLoss: 0.058393\n",
            "Train Epoch: 4 [11200/22400 (50%)]\tLoss: 0.027544\n",
            "Train Epoch: 4 [12800/22400 (57%)]\tLoss: 0.110820\n",
            "Train Epoch: 4 [14400/22400 (64%)]\tLoss: 0.191017\n",
            "Train Epoch: 4 [16000/22400 (71%)]\tLoss: 0.446508\n",
            "Train Epoch: 4 [17600/22400 (79%)]\tLoss: 0.176705\n",
            "Train Epoch: 4 [19200/22400 (86%)]\tLoss: 0.126617\n",
            "Train Epoch: 4 [20800/22400 (93%)]\tLoss: 0.020741\n",
            "Train Epoch: 4 [22400/22400 (100%)]\tLoss: 0.030686\n",
            "\n",
            "Average Val Loss: 0.0392, Val Accuracy: 5534/5600 (98.821%)\n",
            "\n",
            "Train Epoch: 5 [1600/22400 (7%)]\tLoss: 0.200629\n",
            "Train Epoch: 5 [3200/22400 (14%)]\tLoss: 0.027970\n",
            "Train Epoch: 5 [4800/22400 (21%)]\tLoss: 0.280046\n",
            "Train Epoch: 5 [6400/22400 (29%)]\tLoss: 0.106726\n",
            "Train Epoch: 5 [8000/22400 (36%)]\tLoss: 0.054497\n",
            "Train Epoch: 5 [9600/22400 (43%)]\tLoss: 0.034476\n",
            "Train Epoch: 5 [11200/22400 (50%)]\tLoss: 0.022694\n",
            "Train Epoch: 5 [12800/22400 (57%)]\tLoss: 0.458033\n",
            "Train Epoch: 5 [14400/22400 (64%)]\tLoss: 0.723492\n",
            "Train Epoch: 5 [16000/22400 (71%)]\tLoss: 0.837433\n",
            "Train Epoch: 5 [17600/22400 (79%)]\tLoss: 0.089752\n",
            "Train Epoch: 5 [19200/22400 (86%)]\tLoss: 0.043167\n",
            "Train Epoch: 5 [20800/22400 (93%)]\tLoss: 0.028406\n",
            "Train Epoch: 5 [22400/22400 (100%)]\tLoss: 0.062229\n",
            "\n",
            "Average Val Loss: 0.0404, Val Accuracy: 5532/5600 (98.786%)\n",
            "\n",
            "Train Epoch: 6 [1600/22400 (7%)]\tLoss: 0.223440\n",
            "Train Epoch: 6 [3200/22400 (14%)]\tLoss: 0.044671\n",
            "Train Epoch: 6 [4800/22400 (21%)]\tLoss: 0.588908\n",
            "Train Epoch: 6 [6400/22400 (29%)]\tLoss: 0.003070\n",
            "Train Epoch: 6 [8000/22400 (36%)]\tLoss: 0.096530\n",
            "Train Epoch: 6 [9600/22400 (43%)]\tLoss: 0.259881\n",
            "Train Epoch: 6 [11200/22400 (50%)]\tLoss: 0.032297\n",
            "Train Epoch: 6 [12800/22400 (57%)]\tLoss: 0.055230\n",
            "Train Epoch: 6 [14400/22400 (64%)]\tLoss: 0.019777\n",
            "Train Epoch: 6 [16000/22400 (71%)]\tLoss: 0.046486\n",
            "Train Epoch: 6 [17600/22400 (79%)]\tLoss: 0.128634\n",
            "Train Epoch: 6 [19200/22400 (86%)]\tLoss: 0.011229\n",
            "Train Epoch: 6 [20800/22400 (93%)]\tLoss: 0.109371\n",
            "Train Epoch: 6 [22400/22400 (100%)]\tLoss: 0.004452\n",
            "\n",
            "Average Val Loss: 0.0303, Val Accuracy: 5550/5600 (99.107%)\n",
            "\n",
            "Train Epoch: 7 [1600/22400 (7%)]\tLoss: 0.098352\n",
            "Train Epoch: 7 [3200/22400 (14%)]\tLoss: 0.037957\n",
            "Train Epoch: 7 [4800/22400 (21%)]\tLoss: 0.025479\n",
            "Train Epoch: 7 [6400/22400 (29%)]\tLoss: 0.012313\n",
            "Train Epoch: 7 [8000/22400 (36%)]\tLoss: 0.006104\n",
            "Train Epoch: 7 [9600/22400 (43%)]\tLoss: 0.017552\n",
            "Train Epoch: 7 [11200/22400 (50%)]\tLoss: 0.049220\n",
            "Train Epoch: 7 [12800/22400 (57%)]\tLoss: 0.198132\n",
            "Train Epoch: 7 [14400/22400 (64%)]\tLoss: 0.038790\n",
            "Train Epoch: 7 [16000/22400 (71%)]\tLoss: 0.275133\n",
            "Train Epoch: 7 [17600/22400 (79%)]\tLoss: 0.048540\n",
            "Train Epoch: 7 [19200/22400 (86%)]\tLoss: 0.060960\n",
            "Train Epoch: 7 [20800/22400 (93%)]\tLoss: 0.011857\n",
            "Train Epoch: 7 [22400/22400 (100%)]\tLoss: 0.034104\n",
            "\n",
            "Average Val Loss: 0.0295, Val Accuracy: 5549/5600 (99.089%)\n",
            "\n",
            "Train Epoch: 8 [1600/22400 (7%)]\tLoss: 0.055974\n",
            "Train Epoch: 8 [3200/22400 (14%)]\tLoss: 0.024462\n",
            "Train Epoch: 8 [4800/22400 (21%)]\tLoss: 0.078511\n",
            "Train Epoch: 8 [6400/22400 (29%)]\tLoss: 0.029055\n",
            "Train Epoch: 8 [8000/22400 (36%)]\tLoss: 0.229684\n",
            "Train Epoch: 8 [9600/22400 (43%)]\tLoss: 0.202091\n",
            "Train Epoch: 8 [11200/22400 (50%)]\tLoss: 0.013697\n",
            "Train Epoch: 8 [12800/22400 (57%)]\tLoss: 0.025756\n",
            "Train Epoch: 8 [14400/22400 (64%)]\tLoss: 0.526059\n",
            "Train Epoch: 8 [16000/22400 (71%)]\tLoss: 0.037616\n",
            "Train Epoch: 8 [17600/22400 (79%)]\tLoss: 0.123079\n",
            "Train Epoch: 8 [19200/22400 (86%)]\tLoss: 0.009160\n",
            "Train Epoch: 8 [20800/22400 (93%)]\tLoss: 0.078336\n",
            "Train Epoch: 8 [22400/22400 (100%)]\tLoss: 0.039958\n",
            "\n",
            "Average Val Loss: 0.0286, Val Accuracy: 5551/5600 (99.125%)\n",
            "\n",
            "Train Epoch: 9 [1600/22400 (7%)]\tLoss: 0.007640\n",
            "Train Epoch: 9 [3200/22400 (14%)]\tLoss: 0.040716\n",
            "Train Epoch: 9 [4800/22400 (21%)]\tLoss: 0.015845\n",
            "Train Epoch: 9 [6400/22400 (29%)]\tLoss: 0.200865\n",
            "Train Epoch: 9 [8000/22400 (36%)]\tLoss: 0.005885\n",
            "Train Epoch: 9 [9600/22400 (43%)]\tLoss: 0.059686\n",
            "Train Epoch: 9 [11200/22400 (50%)]\tLoss: 0.022838\n",
            "Train Epoch: 9 [12800/22400 (57%)]\tLoss: 0.198001\n",
            "Train Epoch: 9 [14400/22400 (64%)]\tLoss: 0.015038\n",
            "Train Epoch: 9 [16000/22400 (71%)]\tLoss: 0.055640\n",
            "Train Epoch: 9 [17600/22400 (79%)]\tLoss: 0.361287\n",
            "Train Epoch: 9 [19200/22400 (86%)]\tLoss: 0.322434\n",
            "Train Epoch: 9 [20800/22400 (93%)]\tLoss: 0.067381\n",
            "Train Epoch: 9 [22400/22400 (100%)]\tLoss: 0.197573\n",
            "\n",
            "Average Val Loss: 0.0295, Val Accuracy: 5552/5600 (99.143%)\n",
            "\n",
            "Train Epoch: 10 [1600/22400 (7%)]\tLoss: 0.035543\n",
            "Train Epoch: 10 [3200/22400 (14%)]\tLoss: 0.023552\n",
            "Train Epoch: 10 [4800/22400 (21%)]\tLoss: 0.143163\n",
            "Train Epoch: 10 [6400/22400 (29%)]\tLoss: 0.021038\n",
            "Train Epoch: 10 [8000/22400 (36%)]\tLoss: 0.032810\n",
            "Train Epoch: 10 [9600/22400 (43%)]\tLoss: 0.003635\n",
            "Train Epoch: 10 [11200/22400 (50%)]\tLoss: 0.051087\n",
            "Train Epoch: 10 [12800/22400 (57%)]\tLoss: 0.011024\n",
            "Train Epoch: 10 [14400/22400 (64%)]\tLoss: 0.023033\n",
            "Train Epoch: 10 [16000/22400 (71%)]\tLoss: 0.011735\n",
            "Train Epoch: 10 [17600/22400 (79%)]\tLoss: 0.066006\n",
            "Train Epoch: 10 [19200/22400 (86%)]\tLoss: 0.073380\n",
            "Train Epoch: 10 [20800/22400 (93%)]\tLoss: 0.013309\n",
            "Train Epoch: 10 [22400/22400 (100%)]\tLoss: 0.181379\n",
            "\n",
            "Average Val Loss: 0.0286, Val Accuracy: 5555/5600 (99.196%)\n",
            "\n",
            "Train Epoch: 11 [1600/22400 (7%)]\tLoss: 0.027676\n",
            "Train Epoch: 11 [3200/22400 (14%)]\tLoss: 0.006159\n",
            "Train Epoch: 11 [4800/22400 (21%)]\tLoss: 0.008238\n",
            "Train Epoch: 11 [6400/22400 (29%)]\tLoss: 0.025165\n",
            "Train Epoch: 11 [8000/22400 (36%)]\tLoss: 0.009389\n",
            "Train Epoch: 11 [9600/22400 (43%)]\tLoss: 0.015037\n",
            "Train Epoch: 11 [11200/22400 (50%)]\tLoss: 0.067944\n",
            "Train Epoch: 11 [12800/22400 (57%)]\tLoss: 0.276065\n",
            "Train Epoch: 11 [14400/22400 (64%)]\tLoss: 0.399460\n",
            "Train Epoch: 11 [16000/22400 (71%)]\tLoss: 0.057726\n",
            "Train Epoch: 11 [17600/22400 (79%)]\tLoss: 0.139916\n",
            "Train Epoch: 11 [19200/22400 (86%)]\tLoss: 0.754498\n",
            "Train Epoch: 11 [20800/22400 (93%)]\tLoss: 0.022187\n",
            "Train Epoch: 11 [22400/22400 (100%)]\tLoss: 0.087916\n",
            "\n",
            "Average Val Loss: 0.0290, Val Accuracy: 5555/5600 (99.196%)\n",
            "\n",
            "Train Epoch: 12 [1600/22400 (7%)]\tLoss: 0.046578\n",
            "Train Epoch: 12 [3200/22400 (14%)]\tLoss: 0.042042\n",
            "Train Epoch: 12 [4800/22400 (21%)]\tLoss: 0.002286\n",
            "Train Epoch: 12 [6400/22400 (29%)]\tLoss: 0.014455\n",
            "Train Epoch: 12 [8000/22400 (36%)]\tLoss: 0.227816\n",
            "Train Epoch: 12 [9600/22400 (43%)]\tLoss: 0.132050\n",
            "Train Epoch: 12 [11200/22400 (50%)]\tLoss: 0.010414\n",
            "Train Epoch: 12 [12800/22400 (57%)]\tLoss: 0.055340\n",
            "Train Epoch: 12 [14400/22400 (64%)]\tLoss: 0.258116\n",
            "Train Epoch: 12 [16000/22400 (71%)]\tLoss: 0.102793\n",
            "Train Epoch: 12 [17600/22400 (79%)]\tLoss: 0.040486\n",
            "Train Epoch: 12 [19200/22400 (86%)]\tLoss: 0.005630\n",
            "Train Epoch: 12 [20800/22400 (93%)]\tLoss: 0.062829\n",
            "Train Epoch: 12 [22400/22400 (100%)]\tLoss: 0.178646\n",
            "\n",
            "Average Val Loss: 0.0262, Val Accuracy: 5557/5600 (99.232%)\n",
            "\n",
            "Train Epoch: 13 [1600/22400 (7%)]\tLoss: 0.066844\n",
            "Train Epoch: 13 [3200/22400 (14%)]\tLoss: 0.024594\n",
            "Train Epoch: 13 [4800/22400 (21%)]\tLoss: 0.015676\n",
            "Train Epoch: 13 [6400/22400 (29%)]\tLoss: 0.020438\n",
            "Train Epoch: 13 [8000/22400 (36%)]\tLoss: 0.006517\n",
            "Train Epoch: 13 [9600/22400 (43%)]\tLoss: 0.141750\n",
            "Train Epoch: 13 [11200/22400 (50%)]\tLoss: 0.078851\n",
            "Train Epoch: 13 [12800/22400 (57%)]\tLoss: 0.026562\n",
            "Train Epoch: 13 [14400/22400 (64%)]\tLoss: 0.015143\n",
            "Train Epoch: 13 [16000/22400 (71%)]\tLoss: 0.003960\n",
            "Train Epoch: 13 [17600/22400 (79%)]\tLoss: 0.046393\n",
            "Train Epoch: 13 [19200/22400 (86%)]\tLoss: 0.124420\n",
            "Train Epoch: 13 [20800/22400 (93%)]\tLoss: 0.004301\n",
            "Train Epoch: 13 [22400/22400 (100%)]\tLoss: 0.017351\n",
            "\n",
            "Average Val Loss: 0.0288, Val Accuracy: 5550/5600 (99.107%)\n",
            "\n",
            "Train Epoch: 14 [1600/22400 (7%)]\tLoss: 0.024115\n",
            "Train Epoch: 14 [3200/22400 (14%)]\tLoss: 0.021710\n",
            "Train Epoch: 14 [4800/22400 (21%)]\tLoss: 0.061052\n",
            "Train Epoch: 14 [6400/22400 (29%)]\tLoss: 0.115406\n",
            "Train Epoch: 14 [8000/22400 (36%)]\tLoss: 0.029142\n",
            "Train Epoch: 14 [9600/22400 (43%)]\tLoss: 0.008662\n",
            "Train Epoch: 14 [11200/22400 (50%)]\tLoss: 0.150072\n",
            "Train Epoch: 14 [12800/22400 (57%)]\tLoss: 0.403151\n",
            "Train Epoch: 14 [14400/22400 (64%)]\tLoss: 0.016260\n",
            "Train Epoch: 14 [16000/22400 (71%)]\tLoss: 0.005090\n",
            "Train Epoch: 14 [17600/22400 (79%)]\tLoss: 0.020160\n",
            "Train Epoch: 14 [19200/22400 (86%)]\tLoss: 0.049350\n",
            "Train Epoch: 14 [20800/22400 (93%)]\tLoss: 0.096880\n",
            "Train Epoch: 14 [22400/22400 (100%)]\tLoss: 0.021800\n",
            "\n",
            "Average Val Loss: 0.0265, Val Accuracy: 5559/5600 (99.268%)\n",
            "\n",
            "Train Epoch: 15 [1600/22400 (7%)]\tLoss: 0.014970\n",
            "Train Epoch: 15 [3200/22400 (14%)]\tLoss: 0.367777\n",
            "Train Epoch: 15 [4800/22400 (21%)]\tLoss: 0.174039\n",
            "Train Epoch: 15 [6400/22400 (29%)]\tLoss: 0.020507\n",
            "Train Epoch: 15 [8000/22400 (36%)]\tLoss: 0.108229\n",
            "Train Epoch: 15 [9600/22400 (43%)]\tLoss: 0.028734\n",
            "Train Epoch: 15 [11200/22400 (50%)]\tLoss: 0.012604\n",
            "Train Epoch: 15 [12800/22400 (57%)]\tLoss: 2.039200\n",
            "Train Epoch: 15 [14400/22400 (64%)]\tLoss: 0.040598\n",
            "Train Epoch: 15 [16000/22400 (71%)]\tLoss: 0.084047\n",
            "Train Epoch: 15 [17600/22400 (79%)]\tLoss: 0.020872\n",
            "Train Epoch: 15 [19200/22400 (86%)]\tLoss: 0.014113\n",
            "Train Epoch: 15 [20800/22400 (93%)]\tLoss: 0.274024\n",
            "Train Epoch: 15 [22400/22400 (100%)]\tLoss: 0.020396\n",
            "\n",
            "Average Val Loss: 0.0269, Val Accuracy: 5558/5600 (99.250%)\n",
            "\n",
            "Train Epoch: 16 [1600/22400 (7%)]\tLoss: 0.067901\n",
            "Train Epoch: 16 [3200/22400 (14%)]\tLoss: 0.106627\n",
            "Train Epoch: 16 [4800/22400 (21%)]\tLoss: 0.211117\n",
            "Train Epoch: 16 [6400/22400 (29%)]\tLoss: 0.439816\n",
            "Train Epoch: 16 [8000/22400 (36%)]\tLoss: 0.021955\n",
            "Train Epoch: 16 [9600/22400 (43%)]\tLoss: 0.479661\n",
            "Train Epoch: 16 [11200/22400 (50%)]\tLoss: 0.022813\n",
            "Train Epoch: 16 [12800/22400 (57%)]\tLoss: 0.011840\n",
            "Train Epoch: 16 [14400/22400 (64%)]\tLoss: 0.008153\n",
            "Train Epoch: 16 [16000/22400 (71%)]\tLoss: 0.214824\n",
            "Train Epoch: 16 [17600/22400 (79%)]\tLoss: 0.183335\n",
            "Train Epoch: 16 [19200/22400 (86%)]\tLoss: 0.010766\n",
            "Train Epoch: 16 [20800/22400 (93%)]\tLoss: 0.164491\n",
            "Train Epoch: 16 [22400/22400 (100%)]\tLoss: 0.146857\n",
            "\n",
            "Average Val Loss: 0.0262, Val Accuracy: 5556/5600 (99.214%)\n",
            "\n",
            "Train Epoch: 17 [1600/22400 (7%)]\tLoss: 0.544774\n",
            "Train Epoch: 17 [3200/22400 (14%)]\tLoss: 0.008415\n",
            "Train Epoch: 17 [4800/22400 (21%)]\tLoss: 0.229129\n",
            "Train Epoch: 17 [6400/22400 (29%)]\tLoss: 0.007673\n",
            "Train Epoch: 17 [8000/22400 (36%)]\tLoss: 0.136917\n",
            "Train Epoch: 17 [9600/22400 (43%)]\tLoss: 0.130020\n",
            "Train Epoch: 17 [11200/22400 (50%)]\tLoss: 0.004348\n",
            "Train Epoch: 17 [12800/22400 (57%)]\tLoss: 0.100751\n",
            "Train Epoch: 17 [14400/22400 (64%)]\tLoss: 0.004347\n",
            "Train Epoch: 17 [16000/22400 (71%)]\tLoss: 0.049948\n",
            "Train Epoch: 17 [17600/22400 (79%)]\tLoss: 0.018886\n",
            "Train Epoch: 17 [19200/22400 (86%)]\tLoss: 0.179911\n",
            "Train Epoch: 17 [20800/22400 (93%)]\tLoss: 0.121208\n",
            "Train Epoch: 17 [22400/22400 (100%)]\tLoss: 0.547810\n",
            "\n",
            "Average Val Loss: 0.0266, Val Accuracy: 5556/5600 (99.214%)\n",
            "\n",
            "Train Epoch: 18 [1600/22400 (7%)]\tLoss: 0.230362\n",
            "Train Epoch: 18 [3200/22400 (14%)]\tLoss: 0.223843\n",
            "Train Epoch: 18 [4800/22400 (21%)]\tLoss: 0.005369\n",
            "Train Epoch: 18 [6400/22400 (29%)]\tLoss: 0.094408\n",
            "Train Epoch: 18 [8000/22400 (36%)]\tLoss: 0.015226\n",
            "Train Epoch: 18 [9600/22400 (43%)]\tLoss: 0.101168\n",
            "Train Epoch: 18 [11200/22400 (50%)]\tLoss: 0.016596\n",
            "Train Epoch: 18 [12800/22400 (57%)]\tLoss: 0.055263\n",
            "Train Epoch: 18 [14400/22400 (64%)]\tLoss: 0.038713\n",
            "Train Epoch: 18 [16000/22400 (71%)]\tLoss: 0.004092\n",
            "Train Epoch: 18 [17600/22400 (79%)]\tLoss: 0.005976\n",
            "Train Epoch: 18 [19200/22400 (86%)]\tLoss: 0.019934\n",
            "Train Epoch: 18 [20800/22400 (93%)]\tLoss: 0.039170\n",
            "Train Epoch: 18 [22400/22400 (100%)]\tLoss: 0.091055\n",
            "\n",
            "Average Val Loss: 0.0270, Val Accuracy: 5562/5600 (99.321%)\n",
            "\n",
            "Train Epoch: 19 [1600/22400 (7%)]\tLoss: 0.005926\n",
            "Train Epoch: 19 [3200/22400 (14%)]\tLoss: 0.009613\n",
            "Train Epoch: 19 [4800/22400 (21%)]\tLoss: 0.005578\n",
            "Train Epoch: 19 [6400/22400 (29%)]\tLoss: 0.079020\n",
            "Train Epoch: 19 [8000/22400 (36%)]\tLoss: 0.087496\n",
            "Train Epoch: 19 [9600/22400 (43%)]\tLoss: 0.011315\n",
            "Train Epoch: 19 [11200/22400 (50%)]\tLoss: 0.005918\n",
            "Train Epoch: 19 [12800/22400 (57%)]\tLoss: 0.035419\n",
            "Train Epoch: 19 [14400/22400 (64%)]\tLoss: 0.002426\n",
            "Train Epoch: 19 [16000/22400 (71%)]\tLoss: 0.122515\n",
            "Train Epoch: 19 [17600/22400 (79%)]\tLoss: 0.013587\n",
            "Train Epoch: 19 [19200/22400 (86%)]\tLoss: 0.123780\n",
            "Train Epoch: 19 [20800/22400 (93%)]\tLoss: 0.092042\n",
            "Train Epoch: 19 [22400/22400 (100%)]\tLoss: 0.019253\n",
            "\n",
            "Average Val Loss: 0.0265, Val Accuracy: 5556/5600 (99.214%)\n",
            "\n",
            "Train Epoch: 20 [1600/22400 (7%)]\tLoss: 0.015487\n",
            "Train Epoch: 20 [3200/22400 (14%)]\tLoss: 0.193948\n",
            "Train Epoch: 20 [4800/22400 (21%)]\tLoss: 0.259720\n",
            "Train Epoch: 20 [6400/22400 (29%)]\tLoss: 0.027589\n",
            "Train Epoch: 20 [8000/22400 (36%)]\tLoss: 0.108576\n",
            "Train Epoch: 20 [9600/22400 (43%)]\tLoss: 0.047518\n",
            "Train Epoch: 20 [11200/22400 (50%)]\tLoss: 0.055592\n",
            "Train Epoch: 20 [12800/22400 (57%)]\tLoss: 0.121303\n",
            "Train Epoch: 20 [14400/22400 (64%)]\tLoss: 0.199171\n",
            "Train Epoch: 20 [16000/22400 (71%)]\tLoss: 0.012956\n",
            "Train Epoch: 20 [17600/22400 (79%)]\tLoss: 0.115855\n",
            "Train Epoch: 20 [19200/22400 (86%)]\tLoss: 0.202033\n",
            "Train Epoch: 20 [20800/22400 (93%)]\tLoss: 0.016727\n",
            "Train Epoch: 20 [22400/22400 (100%)]\tLoss: 0.010170\n",
            "\n",
            "Average Val Loss: 0.0274, Val Accuracy: 5558/5600 (99.250%)\n",
            "\n",
            "Train Epoch: 21 [1600/22400 (7%)]\tLoss: 0.153030\n",
            "Train Epoch: 21 [3200/22400 (14%)]\tLoss: 0.183643\n",
            "Train Epoch: 21 [4800/22400 (21%)]\tLoss: 0.006309\n",
            "Train Epoch: 21 [6400/22400 (29%)]\tLoss: 0.126111\n",
            "Train Epoch: 21 [8000/22400 (36%)]\tLoss: 0.009890\n",
            "Train Epoch: 21 [9600/22400 (43%)]\tLoss: 0.013947\n",
            "Train Epoch: 21 [11200/22400 (50%)]\tLoss: 0.023793\n",
            "Train Epoch: 21 [12800/22400 (57%)]\tLoss: 0.130314\n",
            "Train Epoch: 21 [14400/22400 (64%)]\tLoss: 0.013715\n",
            "Train Epoch: 21 [16000/22400 (71%)]\tLoss: 0.416957\n",
            "Train Epoch: 21 [17600/22400 (79%)]\tLoss: 0.008289\n",
            "Train Epoch: 21 [19200/22400 (86%)]\tLoss: 0.026202\n",
            "Train Epoch: 21 [20800/22400 (93%)]\tLoss: 0.060926\n",
            "Train Epoch: 21 [22400/22400 (100%)]\tLoss: 0.029291\n",
            "\n",
            "Average Val Loss: 0.0273, Val Accuracy: 5554/5600 (99.179%)\n",
            "\n",
            "Train Epoch: 22 [1600/22400 (7%)]\tLoss: 0.040974\n",
            "Train Epoch: 22 [3200/22400 (14%)]\tLoss: 0.027928\n",
            "Train Epoch: 22 [4800/22400 (21%)]\tLoss: 0.009876\n",
            "Train Epoch: 22 [6400/22400 (29%)]\tLoss: 0.008740\n",
            "Train Epoch: 22 [8000/22400 (36%)]\tLoss: 0.031099\n",
            "Train Epoch: 22 [9600/22400 (43%)]\tLoss: 0.323367\n",
            "Train Epoch: 22 [11200/22400 (50%)]\tLoss: 0.003801\n",
            "Train Epoch: 22 [12800/22400 (57%)]\tLoss: 0.005883\n",
            "Train Epoch: 22 [14400/22400 (64%)]\tLoss: 0.021430\n",
            "Train Epoch: 22 [16000/22400 (71%)]\tLoss: 0.065301\n",
            "Train Epoch: 22 [17600/22400 (79%)]\tLoss: 0.002836\n",
            "Train Epoch: 22 [19200/22400 (86%)]\tLoss: 0.054575\n",
            "Train Epoch: 22 [20800/22400 (93%)]\tLoss: 0.275738\n",
            "Train Epoch: 22 [22400/22400 (100%)]\tLoss: 0.042252\n",
            "\n",
            "Average Val Loss: 0.0271, Val Accuracy: 5557/5600 (99.232%)\n",
            "\n",
            "Train Epoch: 23 [1600/22400 (7%)]\tLoss: 0.010843\n",
            "Train Epoch: 23 [3200/22400 (14%)]\tLoss: 0.056879\n",
            "Train Epoch: 23 [4800/22400 (21%)]\tLoss: 0.137007\n",
            "Train Epoch: 23 [6400/22400 (29%)]\tLoss: 0.023399\n",
            "Train Epoch: 23 [8000/22400 (36%)]\tLoss: 0.120550\n",
            "Train Epoch: 23 [9600/22400 (43%)]\tLoss: 0.009757\n",
            "Train Epoch: 23 [11200/22400 (50%)]\tLoss: 0.010951\n",
            "Train Epoch: 23 [12800/22400 (57%)]\tLoss: 0.050795\n",
            "Train Epoch: 23 [14400/22400 (64%)]\tLoss: 0.047296\n",
            "Train Epoch: 23 [16000/22400 (71%)]\tLoss: 0.703915\n",
            "Train Epoch: 23 [17600/22400 (79%)]\tLoss: 0.015266\n",
            "Train Epoch: 23 [19200/22400 (86%)]\tLoss: 0.297919\n",
            "Train Epoch: 23 [20800/22400 (93%)]\tLoss: 0.012342\n",
            "Train Epoch: 23 [22400/22400 (100%)]\tLoss: 0.007188\n",
            "\n",
            "Average Val Loss: 0.0264, Val Accuracy: 5559/5600 (99.268%)\n",
            "\n",
            "Train Epoch: 24 [1600/22400 (7%)]\tLoss: 0.118548\n",
            "Train Epoch: 24 [3200/22400 (14%)]\tLoss: 0.026261\n",
            "Train Epoch: 24 [4800/22400 (21%)]\tLoss: 0.054043\n",
            "Train Epoch: 24 [6400/22400 (29%)]\tLoss: 0.003421\n",
            "Train Epoch: 24 [8000/22400 (36%)]\tLoss: 0.059255\n",
            "Train Epoch: 24 [9600/22400 (43%)]\tLoss: 0.042173\n",
            "Train Epoch: 24 [11200/22400 (50%)]\tLoss: 0.003578\n",
            "Train Epoch: 24 [12800/22400 (57%)]\tLoss: 0.076258\n",
            "Train Epoch: 24 [14400/22400 (64%)]\tLoss: 0.002950\n",
            "Train Epoch: 24 [16000/22400 (71%)]\tLoss: 0.022980\n",
            "Train Epoch: 24 [17600/22400 (79%)]\tLoss: 0.004559\n",
            "Train Epoch: 24 [19200/22400 (86%)]\tLoss: 0.002077\n",
            "Train Epoch: 24 [20800/22400 (93%)]\tLoss: 0.002890\n",
            "Train Epoch: 24 [22400/22400 (100%)]\tLoss: 0.004396\n",
            "\n",
            "Average Val Loss: 0.0268, Val Accuracy: 5555/5600 (99.196%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}